{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second experiment - second configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "from analysis_tools import load_raw\n",
    "\n",
    "import mne\n",
    "from mne import Epochs, find_events\n",
    "from mne.decoding import Vectorizer\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from pyriemann.estimation import ERPCovariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from pyriemann.classification import MDM\n",
    "from pyriemann.spatialfilters import Xdawn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The generated epochs and authentication datasets will be saved in the \"epochs\" directory. If the directory does not exist, it is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'second_experiment/second_configuration/epochs'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of Notch to attenuate the frequency at 50 Hz, the sixth-order Butterworth band-pass filter with cut-off frequencies of 1-17 Hz, and ICA. After their application, the framework generates the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_by_subject(subject_name):\n",
    "    count = 1\n",
    "    datasets = sorted(glob.glob('data/'+ subject_name + '_*.csv'))\n",
    "    array_epochs = []\n",
    "    for dataset in datasets:\n",
    "        sampling_rate = 256\n",
    "\n",
    "        ch_names = {}\n",
    "        \n",
    "        raw = load_raw(dataset, sfreq=sampling_rate, stim_ind=8, replace_ch_names=None, ch_ind=[0, 1, 2, 3, 4, 5, 6, 7])\n",
    "        \n",
    "        for i, chn in enumerate(raw.ch_names):\n",
    "            ch_names[chn] = i\n",
    "\n",
    "        raw_notch = raw.copy().notch_filter([50.0])\n",
    "\n",
    "        iir_params = dict(order=6, ftype='butter')\n",
    "        raw_notch_and_filter = raw_notch.copy().filter(1, 17, method='iir', iir_params=iir_params)\n",
    "            \n",
    "        ica = mne.preprocessing.ICA(n_components=8, random_state=97)\n",
    "        ica.fit(raw_notch_and_filter)\n",
    "        \n",
    "        raw_notch_and_filter_ica = raw_notch_and_filter.copy()\n",
    "        \n",
    "        ica.exclude = []\n",
    "        eog_inds, eog_scores = ica.find_bads_eog(raw_notch_and_filter_ica, ['Fp1','Fp2'], threshold=1.5)\n",
    "        ica.exclude = eog_inds\n",
    "                \n",
    "        ica.apply(raw_notch_and_filter_ica)\n",
    "\n",
    "        events = find_events(raw_notch_and_filter_ica, shortest_event=1) \n",
    "                \n",
    "        event_id = {'Target': 1, 'NoTarget': 2}\n",
    "        reject = {'eeg': 100e-6}\n",
    "\n",
    "        epochs = Epochs(raw_notch_and_filter_ica, events=events, event_id=event_id, tmin=-0.1, tmax=0.8, reject=reject, preload=True)\n",
    "        epochs.pick_types(eeg=True)\n",
    "    \n",
    "        array_epochs.append(epochs)\n",
    "        \n",
    "        count = count + 1\n",
    "    \n",
    "    return mne.concatenate_epochs(array_epochs, add_offset=True)\n",
    "\n",
    "process_by_subject(\"user_01\").save('second_experiment/second_configuration/epochs/extracted_epochs_user_01_second_experiment_second_configuration-epo.fif', overwrite = True)\n",
    "process_by_subject(\"user_02\").save('second_experiment/second_configuration/epochs/extracted_epochs_user_02_second_experiment_second_configuration-epo.fif', overwrite = True)\n",
    "process_by_subject(\"user_03\").save('second_experiment/second_configuration/epochs/extracted_epochs_user_03_second_experiment_second_configuration-epo.fif', overwrite = True)\n",
    "process_by_subject(\"user_04\").save('second_experiment/second_configuration/epochs/extracted_epochs_user_04_second_experiment_second_configuration-epo.fif', overwrite = True)\n",
    "process_by_subject(\"user_05\").save('second_experiment/second_configuration/epochs/extracted_epochs_user_05_second_experiment_second_configuration-epo.fif', overwrite = True)\n",
    "process_by_subject(\"user_06\").save('second_experiment/second_configuration/epochs/extracted_epochs_user_06_second_experiment_second_configuration-epo.fif', overwrite = True)\n",
    "process_by_subject(\"user_07\").save('second_experiment/second_configuration/epochs/extracted_epochs_user_07_second_experiment_second_configuration-epo.fif', overwrite = True)\n",
    "process_by_subject(\"user_08\").save('second_experiment/second_configuration/epochs/extracted_epochs_user_08_second_experiment_second_configuration-epo.fif', overwrite = True)\n",
    "process_by_subject(\"user_09\").save('second_experiment/second_configuration/epochs/extracted_epochs_user_09_second_experiment_second_configuration-epo.fif', overwrite = True)\n",
    "process_by_subject(\"user_10\").save('second_experiment/second_configuration/epochs/extracted_epochs_user_10_second_experiment_second_configuration-epo.fif', overwrite = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of five authentication datasets for each subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_authentication_epochs(subject_name):    \n",
    "    epochs_user_01 = mne.read_epochs('second_experiment/second_configuration/epochs/extracted_epochs_user_01_second_experiment_second_configuration-epo.fif', preload=False)\n",
    "    epochs_user_02 = mne.read_epochs('second_experiment/second_configuration/epochs/extracted_epochs_user_02_second_experiment_second_configuration-epo.fif', preload=False)\n",
    "    epochs_user_03 = mne.read_epochs('second_experiment/second_configuration/epochs/extracted_epochs_user_03_second_experiment_second_configuration-epo.fif', preload=False)\n",
    "    epochs_user_04 = mne.read_epochs('second_experiment/second_configuration/epochs/extracted_epochs_user_04_second_experiment_second_configuration-epo.fif', preload=False)\n",
    "    epochs_user_05 = mne.read_epochs('second_experiment/second_configuration/epochs/extracted_epochs_user_05_second_experiment_second_configuration-epo.fif', preload=False)\n",
    "    epochs_user_06 = mne.read_epochs('second_experiment/second_configuration/epochs/extracted_epochs_user_06_second_experiment_second_configuration-epo.fif', preload=False)\n",
    "    epochs_user_07 = mne.read_epochs('second_experiment/second_configuration/epochs/extracted_epochs_user_07_second_experiment_second_configuration-epo.fif', preload=False)\n",
    "    epochs_user_08 = mne.read_epochs('second_experiment/second_configuration/epochs/extracted_epochs_user_08_second_experiment_second_configuration-epo.fif', preload=False)\n",
    "    epochs_user_09 = mne.read_epochs('second_experiment/second_configuration/epochs/extracted_epochs_user_09_second_experiment_second_configuration-epo.fif', preload=False)\n",
    "    epochs_user_10 = mne.read_epochs('second_experiment/second_configuration/epochs/extracted_epochs_user_10_second_experiment_second_configuration-epo.fif', preload=False)\n",
    "    \n",
    "    subjects = [\"user_01\", \"user_02\", \"user_03\", \"user_04\", \"user_05\", \"user_06\", \"user_07\", \"user_08\", \"user_09\", \"user_10\"]\n",
    "    epochs_subjects = [epochs_user_01, epochs_user_02, epochs_user_03, epochs_user_04, epochs_user_05, epochs_user_06, epochs_user_07, epochs_user_08, epochs_user_09, epochs_user_10]\n",
    "    \n",
    "    epochs_train = []\n",
    "    epochs_test = []\n",
    "    \n",
    "    index_subject = subjects.index(subject_name)\n",
    "    subject = 0\n",
    "    for epochs in epochs_subjects:\n",
    "        no_targets = np.count_nonzero(epochs.events[:, -1]==2)\n",
    "        \n",
    "        index_no_targets = []\n",
    "        y = epochs.events[:, -1]\n",
    "\n",
    "        while(no_targets != 0):\n",
    "            position = random.randint(0, len(y)-1)\n",
    "            if y[position] == 2 and position not in index_no_targets:\n",
    "                index_no_targets.append(position)\n",
    "                no_targets -= 1\n",
    "\n",
    "        epochs.drop(index_no_targets)\n",
    "        \n",
    "        if subject == index_subject:\n",
    "            epochs_subject = epochs\n",
    "    \n",
    "        subject += 1\n",
    "        \n",
    "    del epochs_subjects[index_subject]\n",
    "\n",
    "    targets_subject = np.count_nonzero(epochs_subject.events[:, -1]==1)\n",
    "    number_rest_subjects = len(epochs_subjects)\n",
    "    targets_rest_subjects = targets_subject // number_rest_subjects\n",
    "    targets_last_subject = (targets_subject - (targets_rest_subjects * number_rest_subjects)) + targets_rest_subjects\n",
    "    \n",
    "    num_epochs_train = int(targets_subject * 0.7)\n",
    "    index_epochs_subject = np.random.permutation(targets_subject)\n",
    "    index_train_subect = index_epochs_subject[:num_epochs_train]\n",
    "    index_test_subject = index_epochs_subject[num_epochs_train:]\n",
    "    \n",
    "    epochs_train_subject = epochs_subject[index_train_subect]\n",
    "    epochs_test_subject = epochs_subject[index_test_subject]\n",
    "    \n",
    "    epochs_train.append(epochs_train_subject)\n",
    "    epochs_test.append(epochs_test_subject)\n",
    "    \n",
    "    number_attackers = 3\n",
    "    \n",
    "    attacker_index = random.sample(range(number_rest_subjects), number_attackers)\n",
    "    \n",
    "    subject = 0\n",
    "    for i, epochs in enumerate(epochs_subjects):\n",
    "        targets = np.count_nonzero(epochs.events[:, -1]==1)\n",
    "        index_leftover_targets = []\n",
    "        y = epochs.events[:, -1]\n",
    "\n",
    "        if(subject == number_rest_subjects - 1):\n",
    "            targets_out = targets - targets_last_subject\n",
    "\n",
    "        else:\n",
    "            targets_out = targets - targets_rest_subjects\n",
    "\n",
    "        while(targets_out != 0):\n",
    "            position = random.randint(0, len(y)-1)\n",
    "            if position not in index_leftover_targets:\n",
    "                index_leftover_targets.append(position)\n",
    "                targets_out -= 1\n",
    "\n",
    "        epochs.drop(index_leftover_targets)\n",
    "\n",
    "        for j in range (len(epochs.events)):\n",
    "            epochs.events[j][2] = 2\n",
    "              \n",
    "        subject += 1\n",
    "        if i in attacker_index:\n",
    "            epochs_test.append(epochs)\n",
    "        else:\n",
    "            epochs_train.append(epochs)\n",
    "            \n",
    "    return mne.concatenate_epochs(epochs_train, add_offset=True), mne.concatenate_epochs(epochs_test, add_offset=True)\n",
    "\n",
    "subjects = [\"user_01\", \"user_02\", \"user_03\", \"user_04\", \"user_05\", \"user_06\", \"user_07\", \"user_08\", \"user_09\", \"user_10\"]\n",
    "\n",
    "for subject in subjects:\n",
    "    for i in range(5):\n",
    "        epochs_train, epochs_test = get_authentication_epochs(subject)\n",
    "        epochs_train.save('second_experiment/second_configuration/epochs/authentication_epochs_train_{}_{}_second_experiment_second_configuration-epo.fif'.format(subject, i), overwrite = True)\n",
    "        epochs_test.save('second_experiment/second_configuration/epochs/authentication_epochs_test_{}_{}_second_experiment_second_configuration-epo.fif'.format(subject, i), overwrite = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The results obtained will be saved in the \"results\" directory. If the directory does not exist, it is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'second_experiment/second_configuration/results'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of a CSV file that will contain the results obtained in the authentication process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['Option/Classifier', 'Classifier1-F1Score', 'Classifier1-EER', 'Classifier1-FAR', 'Classifier1-FRR', 'Classifier2-F1Score', 'Classifier2-EER', 'Classifier2-FAR', 'Classifier2-FRR', 'Classifier3-F1Score', 'Classifier3-EER', 'Classifier3-FAR', 'Classifier3-FRR', 'Classifier4-F1Score', 'Classifier4-EER', 'Classifier4-FAR', 'Classifier4-FRR', 'Classifier5-F1Score', 'Classifier5-EER', 'Classifier5-FAR', 'Classifier5-FRR', 'Classifier6-F1Score', 'Classifier6-EER', 'Classifier6-FAR', 'Classifier6-FRR', 'Classifier7-F1Score', 'Classifier7-EER', 'Classifier7-FAR', 'Classifier7-FRR', 'Classifier8-F1Score', 'Classifier8-EER', 'Classifier8-FAR', 'Classifier8-FRR', 'Classifier9-F1Score', 'Classifier9-EER', 'Classifier9-FAR', 'Classifier9-FRR', 'Classifier10-F1Score', 'Classifier10-EER', 'Classifier10-FAR', 'Classifier10-FRR', 'Classifier11-F1Score', 'Classifier11-EER', 'Classifier11-FAR', 'Classifier11-FRR', 'Classifier12-F1Score', 'Classifier12-EER', 'Classifier12-FAR', 'Classifier12-FRR', 'Classifier13-F1Score', 'Classifier13-EER', 'Classifier13-FAR', 'Classifier13-FRR', 'Classifier14-F1Score', 'Classifier14-EER', 'Classifier14-FAR', 'Classifier14-FRR', 'Classifier15-F1Score', 'Classifier15-EER', 'Classifier15-FAR', 'Classifier15-FRR', 'Classifier16-F1Score', 'Classifier16-EER', 'Classifier16-FAR', 'Classifier16-FRR', 'Classifier17-F1Score', 'Classifier17-EER', 'Classifier17-FAR', 'Classifier17-FRR']\n",
    "with open('second_experiment/second_configuration/results/results_second_experiment_second_configuration.csv', 'w', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    writer.writerow(header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authentication process using binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clfs = OrderedDict()\n",
    "\n",
    "clfs['Clasificador I'] = make_pipeline(Vectorizer(), StandardScaler(), LogisticRegression())\n",
    "clfs['Clasificador II'] = make_pipeline(Vectorizer(), LDA(shrinkage='auto', solver='eigen'))\n",
    "clfs['Clasificador III'] = make_pipeline(Xdawn(2, classes=[1]), Vectorizer(), LDA(shrinkage='auto', solver='eigen'))\n",
    "clfs['Clasificador IV'] = make_pipeline(ERPCovariances(estimator='oas'), TangentSpace(), LogisticRegression())\n",
    "clfs['Clasificador V'] = make_pipeline(ERPCovariances(estimator='oas'), MDM())\n",
    "clfs['Clasificador VI'] = make_pipeline(Vectorizer(), RandomForestClassifier(random_state=42))\n",
    "clfs['Clasificador VII'] = make_pipeline(Vectorizer(), QDA())\n",
    "clfs['Clasificador VIII'] = make_pipeline(Vectorizer(), KNeighborsClassifier(n_neighbors=50))\n",
    "clfs['Clasificador IX'] = make_pipeline(Xdawn(2, classes=[1]), Vectorizer(), RandomForestClassifier(random_state=42))\n",
    "clfs['Clasificador X'] = make_pipeline(ERPCovariances(estimator='oas'), TangentSpace(), RandomForestClassifier(random_state=42))\n",
    "clfs['Clasificador XI'] = make_pipeline(ERPCovariances(estimator='oas'), Vectorizer(), RandomForestClassifier(random_state=42))\n",
    "clfs['Clasificador XII'] = make_pipeline(Xdawn(2, classes=[1]), Vectorizer(), QDA())\n",
    "clfs['Clasificador XIII'] = make_pipeline(ERPCovariances(estimator='oas'), TangentSpace(), QDA())\n",
    "clfs['Clasificador XIV'] = make_pipeline(ERPCovariances(estimator='oas'), Vectorizer(), QDA())\n",
    "clfs['Clasificador XV'] = make_pipeline(Xdawn(2, classes=[1]), Vectorizer(), KNeighborsClassifier(n_neighbors=50))\n",
    "clfs['Clasificador XVI'] = make_pipeline(ERPCovariances(estimator='oas'), TangentSpace(), KNeighborsClassifier(n_neighbors=50))\n",
    "clfs['Clasificador XVII'] = make_pipeline(ERPCovariances(estimator='oas'), Vectorizer(), KNeighborsClassifier(n_neighbors=50))\n",
    "\n",
    "def calculate_eer_far_frr(tp, fp, tn, fn):\n",
    "    if (tp + fp) > 0:\n",
    "        far = fp / (tn + fp)\n",
    "    else:\n",
    "        far = 0.0\n",
    "    \n",
    "    if (tp + fn) > 0:\n",
    "        frr = fn / (tp + fn)\n",
    "    else:\n",
    "        frr = 0.0\n",
    "    \n",
    "    eer = (fp + fn) / (tp + tn + fp + fn)\n",
    "    \n",
    "    return eer, far, frr\n",
    "\n",
    "def authentication_by_subject(subject_name, epochs_train, epochs_test, experiment):\n",
    "   \n",
    "    option = 'Second_experiment_second_configuration_' + subject_name + '_' + str(experiment)\n",
    "    \n",
    "    data = []\n",
    "    data.append(option)\n",
    "    \n",
    "    X_train = epochs_train.get_data() * 1e6\n",
    "    times_train = epochs_train.times\n",
    "    y_train = epochs_train.events[:, -1]\n",
    "    \n",
    "    X_test = epochs_test.get_data() * 1e6\n",
    "    times_test = epochs_test.times\n",
    "    y_test = epochs_test.events[:, -1]\n",
    "        \n",
    "    for m in clfs:\n",
    "        clfs[m].fit(X_train, y_train)\n",
    "        y_pred = clfs[m].predict(X_test)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        f1_score = report['weighted avg']['f1-score']\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "        eer, far, frr = calculate_eer_far_frr(tp, fp, tn, fn)\n",
    "\n",
    "        data.append(round(f1_score, 2))\n",
    "        data.append(round(eer, 2))\n",
    "        data.append(round(far, 2))\n",
    "        data.append(round(frr, 2))\n",
    "            \n",
    "        \n",
    "    with open('second_experiment/second_configuration/results/results_second_experiment_second_configuration.csv', 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "    \n",
    "        writer.writerow(data)\n",
    "        \n",
    "        f.close()\n",
    "              \n",
    "subjects = [\"user_01\", \"user_02\", \"user_03\", \"user_04\", \"user_05\", \"user_06\", \"user_07\", \"user_08\", \"user_09\", \"user_10\"]\n",
    "\n",
    "for subject in subjects:\n",
    "    for i in range(5):\n",
    "        authentication_by_subject(subject, mne.read_epochs('second_experiment/second_configuration/epochs/authentication_epochs_train_{}_{}_second_experiment_second_configuration-epo.fif'.format(subject, i), preload=False), mne.read_epochs('second_experiment/second_configuration/epochs/authentication_epochs_test_{}_{}_second_experiment_second_configuration-epo.fif'.format(subject, i), preload=False), i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
