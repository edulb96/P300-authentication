{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third experiment - second configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "from analysis_tools import load_raw\n",
    "\n",
    "import mne\n",
    "from mne import Epochs, find_events\n",
    "from mne.decoding import Vectorizer\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from pyriemann.estimation import ERPCovariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from pyriemann.classification import MDM\n",
    "from pyriemann.spatialfilters import Xdawn\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The generated epochs and identification datasets will be saved in the \"epochs\" directory. If the directory does not exist, it is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'third_experiment/second_configuration/epochs'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of Notch to attenuate the frequency at 50 Hz, the sixth-order Butterworth band-pass filter with cut-off frequencies of 1-17 Hz, and ICA. After their application, the framework generates the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_by_subject(subject_name):\n",
    "    count = 1\n",
    "    datasets = sorted(glob.glob('data/'+ subject_name + '_*.csv'))\n",
    "    array_epochs = []\n",
    "    for dataset in datasets:\n",
    "        sampling_rate = 256\n",
    "\n",
    "        ch_names = {}\n",
    "        \n",
    "        raw = load_raw(dataset, sfreq=sampling_rate, stim_ind=8, replace_ch_names=None, ch_ind=[0, 1, 2, 3, 4, 5, 6, 7])\n",
    "        \n",
    "        for i, chn in enumerate(raw.ch_names):\n",
    "            ch_names[chn] = i\n",
    "        \n",
    "        raw_notch = raw.copy().notch_filter([50.0])\n",
    "\n",
    "        iir_params = dict(order=6, ftype='butter')\n",
    "        raw_notch_and_filter = raw_notch.copy().filter(1, 17, method='iir', iir_params=iir_params)\n",
    "            \n",
    "        ica = mne.preprocessing.ICA(n_components=8, random_state=97)\n",
    "        ica.fit(raw_notch_and_filter)\n",
    "        \n",
    "        raw_notch_and_filter_ica = raw_notch_and_filter.copy()\n",
    "        \n",
    "        ica.exclude = []\n",
    "        eog_inds, eog_scores = ica.find_bads_eog(raw_notch_and_filter_ica, ['Fp1','Fp2'], threshold=1.5)\n",
    "        ica.exclude = eog_inds\n",
    "        \n",
    "        ica.apply(raw_notch_and_filter_ica)\n",
    "\n",
    "        events = find_events(raw_notch_and_filter_ica, shortest_event=1) \n",
    "                \n",
    "        event_id = {'Target': 1, 'NoTarget': 2}\n",
    "        reject = {'eeg': 100e-6}\n",
    "\n",
    "        epochs = Epochs(raw_notch_and_filter_ica, events=events, event_id=event_id, tmin=-0.1, tmax=0.8, reject=reject, preload=True)\n",
    "        epochs.pick_types(eeg=True)\n",
    "    \n",
    "        array_epochs.append(epochs)\n",
    "        \n",
    "        count = count + 1\n",
    "    \n",
    "    return mne.concatenate_epochs(array_epochs, add_offset=True)\n",
    "\n",
    "process_by_subject(\"user_01\").save('third_experiment/second_configuration/epochs/extracted_epochs_user_01_third_experiment_second_configuration-epo.fif', overwrite = True)\n",
    "process_by_subject(\"user_02\").save('third_experiment/second_configuration/epochs/extracted_epochs_user_02_third_experiment_second_configuration-epo.fif', overwrite = True)\n",
    "process_by_subject(\"user_03\").save('third_experiment/second_configuration/epochs/extracted_epochs_user_03_third_experiment_second_configuration-epo.fif', overwrite = True)\n",
    "process_by_subject(\"user_04\").save('third_experiment/second_configuration/epochs/extracted_epochs_user_04_third_experiment_second_configuration-epo.fif', overwrite = True)\n",
    "process_by_subject(\"user_05\").save('third_experiment/second_configuration/epochs/extracted_epochs_user_05_third_experiment_second_configuration-epo.fif', overwrite = True)\n",
    "process_by_subject(\"user_06\").save('third_experiment/second_configuration/epochs/extracted_epochs_user_06_third_experiment_second_configuration-epo.fif', overwrite = True)\n",
    "process_by_subject(\"user_07\").save('third_experiment/second_configuration/epochs/extracted_epochs_user_07_third_experiment_second_configuration-epo.fif', overwrite = True)\n",
    "process_by_subject(\"user_08\").save('third_experiment/second_configuration/epochs/extracted_epochs_user_08_third_experiment_second_configuration-epo.fif', overwrite = True)\n",
    "process_by_subject(\"user_09\").save('third_experiment/second_configuration/epochs/extracted_epochs_user_09_third_experiment_second_configuration-epo.fif', overwrite = True)\n",
    "process_by_subject(\"user_10\").save('third_experiment/second_configuration/epochs/extracted_epochs_user_10_third_experiment_second_configuration-epo.fif', overwrite = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of five identification datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_identification_epochs():    \n",
    "    epochs_user_01 = mne.read_epochs('third_experiment/second_configuration/epochs/extracted_epochs_user_01_third_experiment_second_configuration-epo.fif', preload=False)\n",
    "    epochs_user_02 = mne.read_epochs('third_experiment/second_configuration/epochs/extracted_epochs_user_02_third_experiment_second_configuration-epo.fif', preload=False)\n",
    "    epochs_user_03 = mne.read_epochs('third_experiment/second_configuration/epochs/extracted_epochs_user_03_third_experiment_second_configuration-epo.fif', preload=False)\n",
    "    epochs_user_04 = mne.read_epochs('third_experiment/second_configuration/epochs/extracted_epochs_user_04_third_experiment_second_configuration-epo.fif', preload=False)\n",
    "    epochs_user_05 = mne.read_epochs('third_experiment/second_configuration/epochs/extracted_epochs_user_05_third_experiment_second_configuration-epo.fif', preload=False)\n",
    "    epochs_user_06 = mne.read_epochs('third_experiment/second_configuration/epochs/extracted_epochs_user_06_third_experiment_second_configuration-epo.fif', preload=False)\n",
    "    epochs_user_07 = mne.read_epochs('third_experiment/second_configuration/epochs/extracted_epochs_user_07_third_experiment_second_configuration-epo.fif', preload=False)\n",
    "    epochs_user_08 = mne.read_epochs('third_experiment/second_configuration/epochs/extracted_epochs_user_08_third_experiment_second_configuration-epo.fif', preload=False)\n",
    "    epochs_user_09 = mne.read_epochs('third_experiment/second_configuration/epochs/extracted_epochs_user_09_third_experiment_second_configuration-epo.fif', preload=False)\n",
    "    epochs_user_10 = mne.read_epochs('third_experiment/second_configuration/epochs/extracted_epochs_user_10_third_experiment_second_configuration-epo.fif', preload=False)\n",
    "    \n",
    "    subjects = [\"user_01\", \"user_02\", \"user_03\", \"user_04\", \"user_05\", \"user_06\", \"user_07\", \"user_08\", \"user_09\", \"user_10\"]\n",
    "    epochs_subjects = [epochs_user_01, epochs_user_02, epochs_user_03, epochs_user_04, epochs_user_05, epochs_user_06, epochs_user_07, epochs_user_08, epochs_user_09, epochs_user_10]\n",
    "   \n",
    "    y_names = []\n",
    "    subject = 0\n",
    "    for epochs in epochs_subjects:\n",
    "        no_targets = np.count_nonzero(epochs.events[:, -1]==2)\n",
    "        \n",
    "        index_no_targets = []\n",
    "        y = epochs.events[:, -1]\n",
    "\n",
    "        while(no_targets != 0):\n",
    "            position = random.randint(0, len(y)-1)\n",
    "            if y[position] == 2 and position not in index_no_targets:\n",
    "                index_no_targets.append(position)\n",
    "                no_targets -= 1\n",
    "\n",
    "        epochs.drop(index_no_targets)\n",
    "    \n",
    "        subject += 1\n",
    "        \n",
    "    minimun_targets = 10000\n",
    "    \n",
    "    for epochs in epochs_subjects:\n",
    "        targets = np.count_nonzero(epochs.events[:, -1]==1)\n",
    "        if (targets < minimun_targets):\n",
    "            minimun_targets = targets\n",
    "    \n",
    "    for epochs in epochs_subjects:\n",
    "        index_leftover_targets = []\n",
    "        targets = np.count_nonzero(epochs.events[:, -1]==1)\n",
    "        y = epochs.events[:, -1]\n",
    "        targets_out = targets - minimun_targets\n",
    "        while(targets_out != 0):\n",
    "            position = random.randint(0, len(y)-1)\n",
    "            if position not in index_leftover_targets:\n",
    "                index_leftover_targets.append(position)\n",
    "                targets_out -= 1\n",
    "                \n",
    "        epochs.drop(index_leftover_targets)\n",
    "                           \n",
    "    subject = 0\n",
    "    for epochs in epochs_subjects:\n",
    "        targets = np.count_nonzero(epochs.events[:, -1]==1)\n",
    "        name_subject = subjects[subject]\n",
    "        \n",
    "        for i in range (targets):\n",
    "            y_names = np.append(y_names, name_subject)\n",
    "                  \n",
    "        subject += 1\n",
    "                \n",
    "    return mne.concatenate_epochs(epochs_subjects, add_offset=True), y_names;\n",
    "    \n",
    "for i in range(5):\n",
    "    epochs, y = get_identification_epochs()\n",
    "    epochs.save('third_experiment/second_configuration/epochs/identification_epochs_{}_third_experiment_second_configuration-epo.fif'.format(i), overwrite = True)\n",
    "\n",
    "df = pd.DataFrame(y)\n",
    "df.to_csv('third_experiment/second_configuration/epochs/y_identification_third_experiment_second_configuration.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The results obtained will be saved in the \"results\" directory. If the directory does not exist, it is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'third_experiment/second_configuration/results'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of a CSV file that will contain the results obtained in the identification process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['Option/Classifier', 'Classifier1-F1Score', 'Classifier1-EER', 'Classifier1-FAR', 'Classifier1-FRR', 'Classifier2-F1Score', 'Classifier2-EER', 'Classifier2-FAR', 'Classifier2-FRR', 'Classifier3-F1Score', 'Classifier3-EER', 'Classifier3-FAR', 'Classifier3-FRR', 'Classifier4-F1Score', 'Classifier4-EER', 'Classifier4-FAR', 'Classifier4-FRR', 'Classifier5-F1Score', 'Classifier5-EER', 'Classifier5-FAR', 'Classifier5-FRR', 'Classifier6-F1Score', 'Classifier6-EER', 'Classifier6-FAR', 'Classifier6-FRR', 'Classifier7-F1Score', 'Classifier7-EER', 'Classifier7-FAR', 'Classifier7-FRR', 'Classifier8-F1Score', 'Classifier8-EER', 'Classifier8-FAR', 'Classifier8-FRR', 'Classifier9-F1Score', 'Classifier9-EER', 'Classifier9-FAR', 'Classifier9-FRR', 'Classifier10-F1Score', 'Classifier10-EER', 'Classifier10-FAR', 'Classifier10-FRR', 'Classifier11-F1Score', 'Classifier11-EER', 'Classifier11-FAR', 'Classifier11-FRR', 'Classifier12-F1Score', 'Classifier12-EER', 'Classifier12-FAR', 'Classifier12-FRR', 'Classifier13-F1Score', 'Classifier13-EER', 'Classifier13-FAR', 'Classifier13-FRR', 'Classifier14-F1Score', 'Classifier14-EER', 'Classifier14-FAR', 'Classifier14-FRR', 'Classifier15-F1Score', 'Classifier15-EER', 'Classifier15-FAR', 'Classifier15-FRR', 'Classifier16-F1Score', 'Classifier16-EER', 'Classifier16-FAR', 'Classifier16-FRR', 'Classifier17-F1Score', 'Classifier17-EER', 'Classifier17-FAR', 'Classifier17-FRR']\n",
    "with open('third_experiment/second_configuration/results/results_third_experiment_second_configuration.csv', 'w', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    writer.writerow(header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identification process using multiclass classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clfs = OrderedDict()\n",
    "\n",
    "clfs['Clasificador I'] = make_pipeline(Vectorizer(), StandardScaler(), LogisticRegression())\n",
    "clfs['Clasificador II'] = make_pipeline(Vectorizer(), LDA(shrinkage='auto', solver='eigen'))\n",
    "clfs['Clasificador III'] = make_pipeline(Xdawn(2, classes=[1]), Vectorizer(), LDA(shrinkage='auto', solver='eigen'))\n",
    "clfs['Clasificador IV'] = make_pipeline(ERPCovariances(estimator='oas'), TangentSpace(), LogisticRegression())\n",
    "clfs['Clasificador V'] = make_pipeline(ERPCovariances(estimator='oas'), MDM())\n",
    "clfs['Clasificador VI'] = make_pipeline(Vectorizer(), RandomForestClassifier(random_state=42))\n",
    "clfs['Clasificador VII'] = make_pipeline(Vectorizer(), QDA())\n",
    "clfs['Clasificador VIII'] = make_pipeline(Vectorizer(), KNeighborsClassifier(n_neighbors=50))\n",
    "clfs['Clasificador IX'] = make_pipeline(Xdawn(2, classes=[1]), Vectorizer(), RandomForestClassifier(random_state=42))\n",
    "clfs['Clasificador X'] = make_pipeline(ERPCovariances(estimator='oas'), TangentSpace(), RandomForestClassifier(random_state=42))\n",
    "clfs['Clasificador XI'] = make_pipeline(ERPCovariances(estimator='oas'), Vectorizer(), RandomForestClassifier(random_state=42))\n",
    "clfs['Clasificador XII'] = make_pipeline(Xdawn(2, classes=[1]), Vectorizer(), QDA())\n",
    "clfs['Clasificador XIII'] = make_pipeline(ERPCovariances(estimator='oas'), TangentSpace(), QDA())\n",
    "clfs['Clasificador XIV'] = make_pipeline(ERPCovariances(estimator='oas'), Vectorizer(), QDA())\n",
    "clfs['Clasificador XV'] = make_pipeline(Xdawn(2, classes=[1]), Vectorizer(), KNeighborsClassifier(n_neighbors=50))\n",
    "clfs['Clasificador XVI'] = make_pipeline(ERPCovariances(estimator='oas'), TangentSpace(), KNeighborsClassifier(n_neighbors=50))\n",
    "clfs['Clasificador XVII'] = make_pipeline(ERPCovariances(estimator='oas'), Vectorizer(), KNeighborsClassifier(n_neighbors=50))\n",
    "\n",
    "def calculate_eer_far_frr(cm):\n",
    "    num_classes = cm.shape[0]\n",
    "    eer_sum = 0.0\n",
    "    far_sum = 0.0\n",
    "    frr_sum = 0.0\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        tp = cm[i, i]\n",
    "        fp = np.sum(cm[:, i]) - tp\n",
    "        fn = np.sum(cm[i, :]) - tp\n",
    "        tn = np.sum(cm) - (tp + fp + fn)\n",
    "        \n",
    "        if (tp + fp) > 0:\n",
    "            far = fp / (tn + fp)\n",
    "        else:\n",
    "            far = 0.0\n",
    "        \n",
    "        if (tp + fn) > 0:\n",
    "            frr = fn / (tp + fn)\n",
    "        else:\n",
    "            frr = 0.0\n",
    "        \n",
    "        eer = (fp + fn) / (tp + tn + fp + fn)\n",
    "        \n",
    "        eer_sum += eer\n",
    "        far_sum += far\n",
    "        frr_sum += frr\n",
    "    \n",
    "    avg_eer = eer_sum / num_classes\n",
    "    avg_far = far_sum / num_classes\n",
    "    avg_frr = frr_sum / num_classes\n",
    "    \n",
    "    return avg_eer, avg_far, avg_frr\n",
    "\n",
    "def inicializate_dict():\n",
    "    results = {\n",
    "        clf_name: {'f1_score': [], 'eer': [], 'far': [], 'frr': []} for clf_name in clfs\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def identification(epochs, y_all_subjects, experiment):   \n",
    "    option = 'Third_experiment_second_configuration_' + str(experiment)\n",
    "        \n",
    "    final_results = []\n",
    "    final_results.append(option)\n",
    "    \n",
    "    results = inicializate_dict()\n",
    "    \n",
    "    X = epochs.get_data() * 1e6\n",
    "    times = epochs.times\n",
    "    y = y_all_subjects\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "            \n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        for m in clfs:\n",
    "            clf = OneVsRestClassifier(clfs[m])\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            report = classification_report(y_test, y_pred, output_dict=True)\n",
    "            f1_score = report['weighted avg']['f1-score']\n",
    "                        \n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            \n",
    "            eer, far, frr = calculate_eer_far_frr(cm)\n",
    "            \n",
    "            results[m]['f1_score'].append(round(f1_score, 2))\n",
    "            results[m]['eer'].append(round(eer, 2))\n",
    "            results[m]['far'].append(round(far, 2))\n",
    "            results[m]['frr'].append(round(frr, 2))\n",
    "            \n",
    "    for classifier, metrics in results.items():\n",
    "        final_results.extend([\n",
    "            f\"{sum(metrics['f1_score']) / len(metrics['f1_score']):.2f}\",  \n",
    "            f\"{sum(metrics['eer']) / len(metrics['eer']):.2f}\",             \n",
    "            f\"{sum(metrics['far']) / len(metrics['far']):.2f}\",              \n",
    "            f\"{sum(metrics['frr']) / len(metrics['frr']):.2f}\"               \n",
    "        ])\n",
    "        \n",
    "    with open('third_experiment/second_configuration/results/results_third_experiment_second_configuration.csv', 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "    \n",
    "        writer.writerow(final_results)\n",
    "        \n",
    "        f.close()\n",
    "              \n",
    "df = pd.read_csv('third_experiment/second_configuration/epochs/y_identification_third_experiment_second_configuration.csv')\n",
    "y_names = df['0']\n",
    "for i in range(5):\n",
    "    identification(mne.read_epochs('third_experiment/second_configuration/epochs/identification_epochs_{}_third_experiment_second_configuration-epo.fif'.format(i), preload=False), y_names, i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
