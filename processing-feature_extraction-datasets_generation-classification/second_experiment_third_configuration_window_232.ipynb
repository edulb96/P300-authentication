{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second experiment - third configuration - window size 232."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "from analysis_tools import load_raw\n",
    "\n",
    "import mne\n",
    "from mne import Epochs, find_events\n",
    "from mne.decoding import Vectorizer\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The generated statistics and authentication datasets will be saved in the \"statistics\" directory. If the directory does not exist, it is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'second_experiment/third_configuration/window_232/statistics'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of Notch to attenuate the frequency at 50 Hz, the sixth-order Butterworth band-pass filter with cut-off frequencies of 1-17 Hz, and ICA. After their application, the framework generates the epochs in Dataframe format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_by_subject(subject_name):\n",
    "    count = 1\n",
    "    datasets = sorted(glob.glob('data/'+ subject_name + '_*.csv'))\n",
    "    df_final = pd.DataFrame()\n",
    "    array_epochs = []\n",
    "    for dataset in datasets:\n",
    "        sampling_rate = 256\n",
    "\n",
    "        ch_names = {}\n",
    "            \n",
    "        raw = load_raw(dataset, sfreq=sampling_rate, stim_ind=8, replace_ch_names=None, ch_ind=[0, 1, 2, 3, 4, 5, 6, 7])\n",
    "\n",
    "        for i, chn in enumerate(raw.ch_names):\n",
    "            ch_names[chn] = i\n",
    "                                \n",
    "        raw_notch = raw.copy().notch_filter([50.0])\n",
    "\n",
    "        iir_params = dict(order=6, ftype='butter')\n",
    "        raw_notch_and_filter = raw_notch.copy().filter(1, 17, method='iir', iir_params=iir_params)\n",
    "        \n",
    "        ica = mne.preprocessing.ICA(n_components=8, random_state=97)\n",
    "        ica.fit(raw_notch_and_filter)\n",
    "        \n",
    "        raw_notch_and_filter_ica = raw_notch_and_filter.copy()\n",
    "        \n",
    "        ica.exclude = []\n",
    "        eog_inds, eog_scores = ica.find_bads_eog(raw_notch_and_filter_ica, ['Fp1','Fp2'], threshold=1.5)\n",
    "        ica.exclude = eog_inds\n",
    "                \n",
    "        ica.apply(raw_notch_and_filter_ica)\n",
    "\n",
    "        events = find_events(raw_notch_and_filter_ica, shortest_event=1) \n",
    "                \n",
    "        event_id = {'Target': 1, 'NoTarget': 2}\n",
    "        reject = {'eeg': 100e-6}\n",
    "\n",
    "        epochs = Epochs(raw_notch_and_filter_ica, events=events, event_id=event_id, tmin=-0.1, tmax=0.8, reject=reject, preload=True)\n",
    "        epochs.pick_types(eeg=True)\n",
    "    \n",
    "        array_epochs.append(epochs)\n",
    "        \n",
    "        if count == 20:\n",
    "            all_epochs = mne.concatenate_epochs(array_epochs, add_offset=True)\n",
    "            df_final = all_epochs.to_data_frame()\n",
    "            no_targets = np.count_nonzero(all_epochs.events[:, -1]==2)\n",
    "        \n",
    "            index_no_targets = []\n",
    "            y = all_epochs.events[:, -1]\n",
    "\n",
    "            while(no_targets != 0):\n",
    "                position = random.randint(0, len(y)-1)\n",
    "                if y[position] == 2 and position not in index_no_targets:\n",
    "                    index_no_targets.append(position)\n",
    "                    no_targets -= 1\n",
    "\n",
    "            all_epochs.drop(index_no_targets)\n",
    "            \n",
    "            df_final_only_targets = all_epochs.to_data_frame()\n",
    "            \n",
    "            df_final.to_csv('second_experiment/third_configuration/window_232/statistics/df_{}.csv'.format(subject_name), index=False)\n",
    "            df_final_only_targets.to_csv('second_experiment/third_configuration/window_232/statistics/df_{}_targets.csv'.format(subject_name), index=False)\n",
    "        \n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "process_by_subject(\"user_01\")\n",
    "process_by_subject(\"user_02\")\n",
    "process_by_subject(\"user_03\")\n",
    "process_by_subject(\"user_04\")\n",
    "process_by_subject(\"user_05\")\n",
    "process_by_subject(\"user_06\")\n",
    "process_by_subject(\"user_07\")\n",
    "process_by_subject(\"user_08\")\n",
    "process_by_subject(\"user_09\")\n",
    "process_by_subject(\"user_10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the statistics using a sliding window size equal to 232."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_stadistical_values(channel, data):    \n",
    "    dicc = dict()\n",
    "\n",
    "    dicc[channel+\"_Mean\"] = np.mean(data[channel])\n",
    "    dicc[channel+\"_variance\"] = np.var(data[channel])\n",
    "    dicc[channel+\"_deviation\"] = np.std(data[channel])\n",
    "    dicc[channel+\"_max\"] = np.max(data[channel])\n",
    "    dicc[channel+\"_summatory\"] = np.sum(data[channel])\n",
    "    dicc[channel+\"_median\"] = np.median(data[channel])\n",
    "    \n",
    "    dfReturned = pd.DataFrame()\n",
    "\n",
    "    dfReturned = dfReturned.append(pd.DataFrame.from_dict(dicc, orient='index'))\n",
    "\n",
    "    dfReturned = dfReturned.transpose()\n",
    "\n",
    "    return dfReturned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aply_all_channels(workDF):\n",
    "\n",
    "    channels = [\"Fp1\",\"Fp2\",\"C3\",\"C4\",\"P7\",\"P8\",\"O1\",\"O2\"]\n",
    "    \n",
    "    window_size = 232\n",
    "\n",
    "    allData = pd.DataFrame()\n",
    "\n",
    "    for i in range(0, workDF.shape[0]):\n",
    "        \n",
    "        if ((i+window_size) > workDF.shape[0]):\n",
    "            break\n",
    "\n",
    "        vectors = workDF.copy().iloc[i:i+window_size]\n",
    "        \n",
    "        allChannels = pd.DataFrame()\n",
    "        \n",
    "        for channel in channels:\n",
    "            aux = get_stadistical_values(channel, vectors)\n",
    "\n",
    "            allChannels = pd.concat([allChannels, aux], axis=1)\n",
    "            \n",
    "        allChannels['Condition'] = 1\n",
    "                \n",
    "        allData = pd.concat([allChannels, allData], axis=0)\n",
    "        \n",
    "    return allData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aply_all_channels(pd.read_csv('second_experiment/third_configuration/window_232/statistics/df_user_01_targets.csv')).to_csv('second_experiment/third_configuration/window_232/statistics/df_statistics_user_01_window_232.csv', index=False)\n",
    "aply_all_channels(pd.read_csv('second_experiment/third_configuration/window_232/statistics/df_user_02_targets.csv')).to_csv('second_experiment/third_configuration/window_232/statistics/df_statistics_user_02_window_232.csv', index=False)\n",
    "aply_all_channels(pd.read_csv('second_experiment/third_configuration/window_232/statistics/df_user_03_targets.csv')).to_csv('second_experiment/third_configuration/window_232/statistics/df_statistics_user_03_window_232.csv', index=False)\n",
    "aply_all_channels(pd.read_csv('second_experiment/third_configuration/window_232/statistics/df_user_04_targets.csv')).to_csv('second_experiment/third_configuration/window_232/statistics/df_statistics_user_04_window_232.csv', index=False)\n",
    "aply_all_channels(pd.read_csv('second_experiment/third_configuration/window_232/statistics/df_user_05_targets.csv')).to_csv('second_experiment/third_configuration/window_232/statistics/df_statistics_user_05_window_232.csv', index=False)\n",
    "aply_all_channels(pd.read_csv('second_experiment/third_configuration/window_232/statistics/df_user_06_targets.csv')).to_csv('second_experiment/third_configuration/window_232/statistics/df_statistics_user_06_window_232.csv', index=False)\n",
    "aply_all_channels(pd.read_csv('second_experiment/third_configuration/window_232/statistics/df_user_07_targets.csv')).to_csv('second_experiment/third_configuration/window_232/statistics/df_statistics_user_07_window_232.csv', index=False)\n",
    "aply_all_channels(pd.read_csv('second_experiment/third_configuration/window_232/statistics/df_user_08_targets.csv')).to_csv('second_experiment/third_configuration/window_232/statistics/df_statistics_user_08_window_232.csv', index=False)\n",
    "aply_all_channels(pd.read_csv('second_experiment/third_configuration/window_232/statistics/df_user_09_targets.csv')).to_csv('second_experiment/third_configuration/window_232/statistics/df_statistics_user_09_window_232.csv', index=False)\n",
    "aply_all_channels(pd.read_csv('second_experiment/third_configuration/window_232/statistics/df_user_10_targets.csv')).to_csv('second_experiment/third_configuration/window_232/statistics/df_statistics_user_10_window_232.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of five authentication datasets for each subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_authentication_statistics(subject_name):    \n",
    "    statistics_user_01 = pd.read_csv('second_experiment/third_configuration/window_232/statistics/df_statistics_user_01_window_232.csv')\n",
    "    statistics_user_02 = pd.read_csv('second_experiment/third_configuration/window_232/statistics/df_statistics_user_02_window_232.csv')\n",
    "    statistics_user_03 = pd.read_csv('second_experiment/third_configuration/window_232/statistics/df_statistics_user_03_window_232.csv')\n",
    "    statistics_user_04 = pd.read_csv('second_experiment/third_configuration/window_232/statistics/df_statistics_user_04_window_232.csv')\n",
    "    statistics_user_05 = pd.read_csv('second_experiment/third_configuration/window_232/statistics/df_statistics_user_05_window_232.csv')\n",
    "    statistics_user_06 = pd.read_csv('second_experiment/third_configuration/window_232/statistics/df_statistics_user_06_window_232.csv')\n",
    "    statistics_user_07 = pd.read_csv('second_experiment/third_configuration/window_232/statistics/df_statistics_user_07_window_232.csv')\n",
    "    statistics_user_08 = pd.read_csv('second_experiment/third_configuration/window_232/statistics/df_statistics_user_08_window_232.csv')\n",
    "    statistics_user_09 = pd.read_csv('second_experiment/third_configuration/window_232/statistics/df_statistics_user_09_window_232.csv')\n",
    "    statistics_user_10 = pd.read_csv('second_experiment/third_configuration/window_232/statistics/df_statistics_user_10_window_232.csv')\n",
    "    \n",
    "    subjects = [\"user_01\", \"user_02\", \"user_03\", \"user_04\", \"user_05\", \"user_06\", \"user_07\", \"user_08\", \"user_09\", \"user_10\"]\n",
    "    statistics_subjects = [statistics_user_01, statistics_user_02, statistics_user_03, statistics_user_04, statistics_user_05, statistics_user_06, statistics_user_07, statistics_user_08, statistics_user_09, statistics_user_10]\n",
    "   \n",
    "    statistics_train = pd.DataFrame()\n",
    "    statistics_test = pd.DataFrame()\n",
    "   \n",
    "    new_statistics_targets_only = []\n",
    "    \n",
    "    index_subject = subjects.index(subject_name)\n",
    "    subject = 0\n",
    "        \n",
    "    for statistic in statistics_subjects:\n",
    "        if subject == index_subject:\n",
    "            statistics_subject = statistic\n",
    "            \n",
    "        else:\n",
    "            new_statistics_targets_only.append(statistic)\n",
    "    \n",
    "        subject += 1\n",
    "    \n",
    "    targets_subject = statistics_subject.shape[0]\n",
    "    number_rest_subjects = len(new_statistics_targets_only)\n",
    "    targets_rest_subjects = targets_subject // number_rest_subjects\n",
    "    targets_last_subject = (targets_subject - (targets_rest_subjects * number_rest_subjects)) + targets_rest_subjects\n",
    "    \n",
    "    num_statistics_train = int(targets_subject * 0.7)\n",
    "    index_statistics_subject = np.random.permutation(targets_subject)\n",
    "    index_train_subject = index_statistics_subject[:num_statistics_train]\n",
    "    index_test_subject = index_statistics_subject[num_statistics_train:]\n",
    "    \n",
    "\n",
    "    statistics_train_subject = statistics_subject.iloc[index_train_subject]\n",
    "    statistics_test_subject = statistics_subject.iloc[index_test_subject]\n",
    "    \n",
    "    statistics_test = statistics_test.append(statistics_test_subject, ignore_index=True)\n",
    "    statistics_test = statistics_test.reset_index(drop=True)\n",
    "    \n",
    "    statistics_train = statistics_train.append(statistics_train_subject, ignore_index=True)\n",
    "    statistics_train = statistics_train.reset_index(drop=True)\n",
    "    \n",
    "    number_attackers = 3\n",
    "    \n",
    "    attacker_index = random.sample(range(number_rest_subjects), number_attackers)\n",
    "    \n",
    "    subject = 0\n",
    "    for i, statistic in enumerate(new_statistics_targets_only):     \n",
    "        if(subject == number_rest_subjects - 1):\n",
    "            targets_selected = targets_last_subject\n",
    "\n",
    "        else:\n",
    "            targets_selected = targets_rest_subjects\n",
    "                    \n",
    "        statistics_selected = statistic.sample(n=targets_selected, replace=False)\n",
    "        statistics_selected['Condition'] = 0\n",
    "                        \n",
    "        subject += 1\n",
    "        \n",
    "        if i in attacker_index:\n",
    "            statistics_test = statistics_test.append(statistics_selected, ignore_index=True)\n",
    "            statistics_test = statistics_test.reset_index(drop=True)\n",
    "        else:\n",
    "            statistics_train = statistics_train.append(statistics_selected, ignore_index=True)\n",
    "            statistics_train = statistics_train.reset_index(drop=True)\n",
    "            \n",
    "    return statistics_train, statistics_test   \n",
    "    \n",
    "subjects = [\"user_01\", \"user_02\", \"user_03\", \"user_04\", \"user_05\", \"user_06\", \"user_07\", \"user_08\", \"user_09\", \"user_10\"]\n",
    "\n",
    "for subject in subjects:\n",
    "    for i in range(5):\n",
    "        statistics_train, statistics_test = get_authentication_statistics(subject)\n",
    "        statistics_train.to_csv('second_experiment/third_configuration/window_232/statistics/authentication_statistics_train_{}_{}_second_experiment_third_configuration_window_232.csv'.format(subject, i))\n",
    "        statistics_test.to_csv('second_experiment/third_configuration/window_232/statistics/authentication_statistics_test_{}_{}_second_experiment_third_configuration_window_232.csv'.format(subject, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The results obtained will be saved in the \"results\" directory. If the directory does not exist, it is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'second_experiment/third_configuration/window_232/results'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of a CSV file that will contain the results obtained in the authentication process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['Option/Classifier', 'Classifier1-F1Score', 'Classifier1-EER', 'Classifier1-FAR', 'Classifier1-FRR', 'Classifier2-F1Score', 'Classifier2-EER', 'Classifier2-FAR', 'Classifier2-FRR', 'Classifier6-F1Score', 'Classifier6-EER', 'Classifier6-FAR', 'Classifier6-FRR', 'Classifier7-F1Score', 'Classifier7-EER', 'Classifier7-FAR', 'Classifier7-FRR', 'Classifier8-F1Score', 'Classifier8-EER', 'Classifier8-FAR', 'Classifier8-FRR']\n",
    "with open('second_experiment/third_configuration/window_232/results/results_second_experiment_third_configuration_window_232.csv', 'w', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    writer.writerow(header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authentication process using multiclass classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clfs = OrderedDict()\n",
    "\n",
    "clfs['Clasificador I'] = make_pipeline(Vectorizer(), StandardScaler(), LogisticRegression())\n",
    "clfs['Clasificador II'] = make_pipeline(Vectorizer(), LDA(shrinkage='auto', solver='eigen'))\n",
    "clfs['Clasificador VI'] = make_pipeline(Vectorizer(), RandomForestClassifier(random_state=42))\n",
    "clfs['Clasificador VII'] = make_pipeline(Vectorizer(), QDA())\n",
    "clfs['Clasificador VIII'] = make_pipeline(Vectorizer(), KNeighborsClassifier(n_neighbors=50))\n",
    "\n",
    "def calculate_eer_far_frr(tp, fp, tn, fn):\n",
    "    if (tp + fp) > 0:\n",
    "        far = fp / (tn + fp)\n",
    "    else:\n",
    "        far = 0.0\n",
    "    \n",
    "    if (tp + fn) > 0:\n",
    "        frr = fn / (tp + fn)\n",
    "    else:\n",
    "        frr = 0.0\n",
    "    \n",
    "    eer = (fp + fn) / (tp + tn + fp + fn)\n",
    "    \n",
    "    return eer, far, frr\n",
    "\n",
    "def authentication_by_subject(subject_name, statistics_train, statistics_test, experiment):\n",
    "   \n",
    "    option = 'Second_experiment_third_configuration_window_232_' + subject_name + '_' + str(experiment)\n",
    "    \n",
    "    data = []\n",
    "    data.append(option)\n",
    "        \n",
    "    channels_train = statistics_train.loc[:, \"Fp1_Mean\":\"O2_median\"]\n",
    "    X_train = channels_train.to_numpy()\n",
    "    conditions_train = statistics_train.loc[:, \"Condition\"]\n",
    "    y_train = conditions_train.to_numpy()\n",
    "    \n",
    "    channels_test = statistics_test.loc[:, \"Fp1_Mean\":\"O2_median\"]\n",
    "    X_test = channels_test.to_numpy()\n",
    "    conditions_test = statistics_test.loc[:, \"Condition\"]\n",
    "    y_test = conditions_test.to_numpy()\n",
    "        \n",
    "    for m in clfs:\n",
    "        clfs[m].fit(X_train, y_train)\n",
    "        y_pred = clfs[m].predict(X_test)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        f1_score = report['weighted avg']['f1-score']\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "        eer, far, frr = calculate_eer_far_frr(tp, fp, tn, fn)\n",
    "\n",
    "        data.append(round(f1_score, 2))\n",
    "        data.append(round(eer, 2))\n",
    "        data.append(round(far, 2))\n",
    "        data.append(round(frr, 2))\n",
    "       \n",
    "    with open('second_experiment/third_configuration/window_232/results/results_second_experiment_third_configuration_window_232.csv', 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "    \n",
    "        writer.writerow(data)\n",
    "        \n",
    "        f.close()        \n",
    "\n",
    "subjects = [\"user_01\", \"user_02\", \"user_03\", \"user_04\", \"user_05\", \"user_06\", \"user_07\", \"user_08\", \"user_09\", \"user_10\"]\n",
    "\n",
    "for subject in subjects:\n",
    "    for i in range(5):\n",
    "        authentication_by_subject(subject, pd.read_csv('second_experiment/third_configuration/window_232/statistics/authentication_statistics_train_{}_{}_second_experiment_third_configuration_window_232.csv'.format(subject, i)), pd.read_csv('second_experiment/third_configuration/window_232/statistics/authentication_statistics_test_{}_{}_second_experiment_third_configuration_window_232.csv'.format(subject, i)), i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
