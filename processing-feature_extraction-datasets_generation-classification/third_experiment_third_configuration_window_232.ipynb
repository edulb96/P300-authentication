{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second experiment - third configuration - window 232."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "from analysis_tools import load_raw\n",
    "\n",
    "import mne\n",
    "from mne import Epochs, find_events\n",
    "from mne.decoding import Vectorizer\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The generated statistics and identification datasets will be saved in the \"statistics\" directory. If the directory does not exist, it is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'third_experiment/third_configuration/window_232/statistics'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of Notch to attenuate the frequency at 50 Hz, the sixth-order Butterworth band-pass filter with cut-off frequencies of 1-17 Hz, and ICA. After their application, the framework generates the epochs in Dataframe format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_by_subject(subject_name):\n",
    "    count = 1\n",
    "    datasets = sorted(glob.glob('data/'+ subject_name + '_*.csv'))\n",
    "    df_final = pd.DataFrame()\n",
    "    array_epochs = []\n",
    "    for dataset in datasets:\n",
    "        sampling_rate = 256\n",
    "\n",
    "        ch_names = {}\n",
    "        \n",
    "        raw = load_raw(dataset, sfreq=sampling_rate, stim_ind=8, replace_ch_names=None, ch_ind=[0, 1, 2, 3, 4, 5, 6, 7])\n",
    "        \n",
    "        for i, chn in enumerate(raw.ch_names):\n",
    "            ch_names[chn] = i\n",
    "\n",
    "        raw_notch = raw.copy().notch_filter([50.0])\n",
    "\n",
    "        iir_params = dict(order=6, ftype='butter')\n",
    "        raw_notch_and_filter = raw_notch.copy().filter(1, 17, method='iir', iir_params=iir_params)\n",
    "\n",
    "        ica = mne.preprocessing.ICA(n_components=8, random_state=97)\n",
    "        ica.fit(raw_notch_and_filter)\n",
    "        \n",
    "        raw_notch_and_filter_ica = raw_notch_and_filter.copy()\n",
    "        \n",
    "        ica.exclude = []\n",
    "        eog_inds, eog_scores = ica.find_bads_eog(raw_notch_and_filter_ica, ['Fp1','Fp2'], threshold=1.5)\n",
    "        ica.exclude = eog_inds\n",
    "                \n",
    "        ica.apply(raw_notch_and_filter_ica)\n",
    "\n",
    "        events = find_events(raw_notch_and_filter_ica, shortest_event=1) \n",
    "                \n",
    "        event_id = {'Target': 1, 'NoTarget': 2}\n",
    "        reject = {'eeg': 100e-6}\n",
    "\n",
    "        epochs = Epochs(raw_notch_and_filter_ica, events=events, event_id=event_id, tmin=-0.1, tmax=0.8, reject=reject, preload=True)\n",
    "        epochs.pick_types(eeg=True)\n",
    "    \n",
    "        array_epochs.append(epochs)\n",
    "        \n",
    "        if count == 20:\n",
    "            all_epochs = mne.concatenate_epochs(array_epochs, add_offset=True)\n",
    "            df_final = all_epochs.to_data_frame()\n",
    "            no_targets = np.count_nonzero(all_epochs.events[:, -1]==2)\n",
    "        \n",
    "            index_no_targets = []\n",
    "            y = all_epochs.events[:, -1]\n",
    "\n",
    "            while(no_targets != 0):\n",
    "                position = random.randint(0, len(y)-1)\n",
    "                if y[position] == 2 and position not in index_no_targets:\n",
    "                    index_no_targets.append(position)\n",
    "                    no_targets -= 1\n",
    "\n",
    "            all_epochs.drop(index_no_targets)\n",
    "            \n",
    "            df_final_only_targets = all_epochs.to_data_frame()\n",
    "            \n",
    "            df_final.to_csv('third_experiment/third_configuration/window_232/statistics/df_{}.csv'.format(subject_name), index=False)\n",
    "            df_final_only_targets.to_csv('third_experiment/third_configuration/window_232/statistics/df_{}_targets.csv'.format(subject_name), index=False)\n",
    "        \n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "process_by_subject(\"user_01\")\n",
    "process_by_subject(\"user_02\")\n",
    "process_by_subject(\"user_03\")\n",
    "process_by_subject(\"user_04\")\n",
    "process_by_subject(\"user_05\")\n",
    "process_by_subject(\"user_06\")\n",
    "process_by_subject(\"user_07\")\n",
    "process_by_subject(\"user_08\")\n",
    "process_by_subject(\"user_09\")\n",
    "process_by_subject(\"user_10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the statistics using a sliding window size equal to 232."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stadistical_values(channel, data):    \n",
    "    dicc = dict()\n",
    "\n",
    "    dicc[channel+\"_Mean\"] = np.mean(data[channel])\n",
    "    dicc[channel+\"_variance\"] = np.var(data[channel])\n",
    "    dicc[channel+\"_deviation\"] = np.std(data[channel])\n",
    "    dicc[channel+\"_max\"] = np.max(data[channel])\n",
    "    dicc[channel+\"_summatory\"] = np.sum(data[channel])\n",
    "    dicc[channel+\"_median\"] = np.median(data[channel])\n",
    "\n",
    "    dfReturned = pd.DataFrame()\n",
    "\n",
    "    dfReturned = dfReturned.append(pd.DataFrame.from_dict(dicc, orient='index'))\n",
    "\n",
    "    dfReturned = dfReturned.transpose()\n",
    "\n",
    "    return dfReturned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aply_all_channels(workDF):\n",
    "\n",
    "    channels = [\"Fp1\",\"Fp2\",\"C3\",\"C4\",\"P7\",\"P8\",\"O1\",\"O2\"]\n",
    "    \n",
    "    window_size = 232\n",
    "\n",
    "    allData = pd.DataFrame()\n",
    "\n",
    "    for i in range(0, workDF.shape[0]):\n",
    "        \n",
    "        if ((i+window_size) > workDF.shape[0]):\n",
    "            break\n",
    "\n",
    "        vectors = workDF.copy().iloc[i:i+window_size]\n",
    "        \n",
    "        allChannels = pd.DataFrame()\n",
    "        \n",
    "        for channel in channels:\n",
    "            aux = get_stadistical_values(channel, vectors)\n",
    "\n",
    "            allChannels = pd.concat([allChannels, aux], axis=1)\n",
    "            \n",
    "        allChannels['Condition'] = 1\n",
    "                \n",
    "        allData = pd.concat([allChannels, allData], axis=0)\n",
    "        \n",
    "    return allData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aply_all_channels(pd.read_csv('third_experiment/third_configuration/window_232/statistics/df_user_01_targets.csv')).to_csv('third_experiment/third_configuration/window_232/statistics/df_statistics_user_01_window_232.csv', index=False)\n",
    "aply_all_channels(pd.read_csv('third_experiment/third_configuration/window_232/statistics/df_user_02_targets.csv')).to_csv('third_experiment/third_configuration/window_232/statistics/df_statistics_user_02_window_232.csv', index=False)\n",
    "aply_all_channels(pd.read_csv('third_experiment/third_configuration/window_232/statistics/df_user_03_targets.csv')).to_csv('third_experiment/third_configuration/window_232/statistics/df_statistics_user_03_window_232.csv', index=False)\n",
    "aply_all_channels(pd.read_csv('third_experiment/third_configuration/window_232/statistics/df_user_04_targets.csv')).to_csv('third_experiment/third_configuration/window_232/statistics/df_statistics_user_04_window_232.csv', index=False)\n",
    "aply_all_channels(pd.read_csv('third_experiment/third_configuration/window_232/statistics/df_user_05_targets.csv')).to_csv('third_experiment/third_configuration/window_232/statistics/df_statistics_user_05_window_232.csv', index=False)\n",
    "aply_all_channels(pd.read_csv('third_experiment/third_configuration/window_232/statistics/df_user_06_targets.csv')).to_csv('third_experiment/third_configuration/window_232/statistics/df_statistics_user_06_window_232.csv', index=False)\n",
    "aply_all_channels(pd.read_csv('third_experiment/third_configuration/window_232/statistics/df_user_07_targets.csv')).to_csv('third_experiment/third_configuration/window_232/statistics/df_statistics_user_07_window_232.csv', index=False)\n",
    "aply_all_channels(pd.read_csv('third_experiment/third_configuration/window_232/statistics/df_user_08_targets.csv')).to_csv('third_experiment/third_configuration/window_232/statistics/df_statistics_user_08_window_232.csv', index=False)\n",
    "aply_all_channels(pd.read_csv('third_experiment/third_configuration/window_232/statistics/df_user_09_targets.csv')).to_csv('third_experiment/third_configuration/window_232/statistics/df_statistics_user_09_window_232.csv', index=False)\n",
    "aply_all_channels(pd.read_csv('third_experiment/third_configuration/window_232/statistics/df_user_10_targets.csv')).to_csv('third_experiment/third_configuration/window_232/statistics/df_statistics_user_10_window_232.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of five identification datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_identification_statistics():    \n",
    "    statistics_user_01 = pd.read_csv('third_experiment/third_configuration/window_232/statistics/df_statistics_user_01_window_232.csv')\n",
    "    statistics_user_02 = pd.read_csv('third_experiment/third_configuration/window_232/statistics/df_statistics_user_02_window_232.csv')\n",
    "    statistics_user_03 = pd.read_csv('third_experiment/third_configuration/window_232/statistics/df_statistics_user_03_window_232.csv')\n",
    "    statistics_user_04 = pd.read_csv('third_experiment/third_configuration/window_232/statistics/df_statistics_user_04_window_232.csv')\n",
    "    statistics_user_05 = pd.read_csv('third_experiment/third_configuration/window_232/statistics/df_statistics_user_05_window_232.csv')\n",
    "    statistics_user_06 = pd.read_csv('third_experiment/third_configuration/window_232/statistics/df_statistics_user_06_window_232.csv')\n",
    "    statistics_user_07 = pd.read_csv('third_experiment/third_configuration/window_232/statistics/df_statistics_user_07_window_232.csv')\n",
    "    statistics_user_08 = pd.read_csv('third_experiment/third_configuration/window_232/statistics/df_statistics_user_08_window_232.csv')\n",
    "    statistics_user_09 = pd.read_csv('third_experiment/third_configuration/window_232/statistics/df_statistics_user_09_window_232.csv')\n",
    "    statistics_user_10 = pd.read_csv('third_experiment/third_configuration/window_232/statistics/df_statistics_user_10_window_232.csv')\n",
    "    \n",
    "    subjects = [\"user_01\", \"user_02\", \"user_03\", \"user_04\", \"user_05\", \"user_06\", \"user_07\", \"user_08\", \"user_09\", \"user_10\"]\n",
    "    statistics_subjects = [statistics_user_01, statistics_user_02, statistics_user_03, statistics_user_04, statistics_user_05, statistics_user_06, statistics_user_07, statistics_user_08, statistics_user_09, statistics_user_10]\n",
    "   \n",
    "    new_statistics_targets_only = []\n",
    "    \n",
    "    subject = 0\n",
    "            \n",
    "    for statistic in statistics_subjects:\n",
    "        new_statistics_targets_only.append(statistic)\n",
    "    \n",
    "        subject += 1\n",
    "        \n",
    "    minimun_targets = 1000000000\n",
    "    subject = 0\n",
    "    subject_minimun_targets = 0\n",
    "    for statistic in new_statistics_targets_only:\n",
    "        targets = statistic.shape[0]\n",
    "        if (targets < minimun_targets):\n",
    "            minimun_targets = targets\n",
    "            subject_minimun_targets = subject\n",
    "        subject += 1\n",
    "        \n",
    "    name_subject_minimun_targets = subjects[subject_minimun_targets] \n",
    "        \n",
    "    statistics_subject_minimun_targets = statistics_subjects[subject_minimun_targets]\n",
    "    list_name_subject_minimun_targets = [name_subject_minimun_targets] * minimun_targets\n",
    "    statistics_subject_minimun_targets['Condition'] = list_name_subject_minimun_targets\n",
    "    \n",
    "    statistics_multiclass = pd.DataFrame()\n",
    "    subject = 0\n",
    "    for statistic in new_statistics_targets_only:\n",
    "        if (subject != subject_minimun_targets):\n",
    "            name_subject = subjects[subject]\n",
    "            targets_statistic = statistic.shape[0]\n",
    "            index_selected_targets = random.sample(range(targets_statistic), minimun_targets)\n",
    "            targets_selected = statistic.iloc[index_selected_targets]\n",
    "\n",
    "            name_subject = subjects[subject]\n",
    "            list_name_subject = [name_subject] * minimun_targets\n",
    "            targets_selected['Condition'] = list_name_subject\n",
    "            statistics_multiclass = statistics_multiclass.append(targets_selected, ignore_index=True)\n",
    "            statistics_multiclass = statistics_multiclass.reset_index(drop=True)\n",
    "                    \n",
    "        else:\n",
    "            statistics_multiclass = statistics_multiclass.append(statistics_subject_minimun_targets, ignore_index=True)\n",
    "            statistics_multiclass = statistics_multiclass.reset_index(drop=True)\n",
    "        \n",
    "        subject += 1\n",
    "        \n",
    "    return statistics_multiclass\n",
    "\n",
    "for i in range(5):\n",
    "    get_identification_statistics().to_csv('third_experiment/third_configuration/window_232/statistics/identification_statistics_{}_third_experiment_third_configuration_window_232.csv'.format(i), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The results obtained will be saved in the \"results\" directory. If the directory does not exist, it is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'third_experiment/third_configuration/window_232/results'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of a CSV file that will contain the results obtained in the identification process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['Option/Classifier', 'Classifier1-F1Score', 'Classifier1-EER', 'Classifier1-FAR', 'Classifier1-FRR', 'Classifier2-F1Score', 'Classifier2-EER', 'Classifier2-FAR', 'Classifier2-FRR', 'Classifier6-F1Score', 'Classifier6-EER', 'Classifier6-FAR', 'Classifier6-FRR', 'Classifier7-F1Score', 'Classifier7-EER', 'Classifier7-FAR', 'Classifier7-FRR', 'Classifier8-F1Score', 'Classifier8-EER', 'Classifier8-FAR', 'Classifier8-FRR']\n",
    "with open('third_experiment/third_configuration/window_232/results/results_third_experiment_third_configuration_window_232.csv', 'w', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    writer.writerow(header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identification process using multiclass classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clfs = OrderedDict()\n",
    "\n",
    "clfs['Clasificador I'] = make_pipeline(Vectorizer(), StandardScaler(), LogisticRegression())\n",
    "clfs['Clasificador II'] = make_pipeline(Vectorizer(), LDA(shrinkage='auto', solver='eigen'))\n",
    "clfs['Clasificador VI'] = make_pipeline(Vectorizer(), RandomForestClassifier(random_state=42))\n",
    "clfs['Clasificador VII'] = make_pipeline(Vectorizer(), QDA())\n",
    "clfs['Clasificador VIII'] = make_pipeline(Vectorizer(), KNeighborsClassifier(n_neighbors=50))\n",
    "\n",
    "def calculate_eer_far_frr(cm):\n",
    "    num_classes = cm.shape[0]\n",
    "    eer_sum = 0.0\n",
    "    far_sum = 0.0\n",
    "    frr_sum = 0.0\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        tp = cm[i, i]\n",
    "        fp = np.sum(cm[:, i]) - tp\n",
    "        fn = np.sum(cm[i, :]) - tp\n",
    "        tn = np.sum(cm) - (tp + fp + fn)\n",
    "        \n",
    "        if (tp + fp) > 0:\n",
    "            far = fp / (tn + fp)\n",
    "        else:\n",
    "            far = 0.0\n",
    "        \n",
    "        if (tp + fn) > 0:\n",
    "            frr = fn / (tp + fn)\n",
    "        else:\n",
    "            frr = 0.0\n",
    "        \n",
    "        eer = (fp + fn) / (tp + tn + fp + fn)\n",
    "        \n",
    "        eer_sum += eer\n",
    "        far_sum += far\n",
    "        frr_sum += frr\n",
    "    \n",
    "    avg_eer = eer_sum / num_classes\n",
    "    avg_far = far_sum / num_classes\n",
    "    avg_frr = frr_sum / num_classes\n",
    "    \n",
    "    return avg_eer, avg_far, avg_frr\n",
    "\n",
    "def inicializate_dict():\n",
    "    results = {\n",
    "        clf_name: {'f1_score': [], 'eer': [], 'far': [], 'frr': []} for clf_name in clfs\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def identification(statistics, experiment):\n",
    "   \n",
    "    option = 'Third_experiment_third_configuration_window_232_' + str(experiment)\n",
    "    \n",
    "    final_results = []\n",
    "    final_results.append(option)\n",
    "    \n",
    "    results = inicializate_dict()\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "        \n",
    "    channels = statistics.loc[:, \"Fp1_Mean\":\"O2_median\"]\n",
    "    X = channels.to_numpy()\n",
    "    conditions = statistics.loc[:, \"Condition\"]\n",
    "    y = conditions.to_numpy()\n",
    "           \n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "            \n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        for m in clfs:\n",
    "            clf = OneVsRestClassifier(clfs[m])\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            report = classification_report(y_test, y_pred, output_dict=True)\n",
    "            f1_score = report['weighted avg']['f1-score']\n",
    "                        \n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            \n",
    "            eer, far, frr = calculate_eer_far_frr(cm)\n",
    "            \n",
    "            results[m]['f1_score'].append(round(f1_score, 2))\n",
    "            results[m]['eer'].append(round(eer, 2))\n",
    "            results[m]['far'].append(round(far, 2))\n",
    "            results[m]['frr'].append(round(frr, 2))\n",
    "            \n",
    "    for classifier, metrics in results.items():\n",
    "        final_results.extend([\n",
    "            f\"{sum(metrics['f1_score']) / len(metrics['f1_score']):.2f}\",  \n",
    "            f\"{sum(metrics['eer']) / len(metrics['eer']):.2f}\",             \n",
    "            f\"{sum(metrics['far']) / len(metrics['far']):.2f}\",              \n",
    "            f\"{sum(metrics['frr']) / len(metrics['frr']):.2f}\"               \n",
    "        ])\n",
    "       \n",
    "    with open('third_experiment/third_configuration/window_232/results/results_third_experiment_third_configuration_window_232.csv', 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "    \n",
    "        writer.writerow(final_results)\n",
    "        \n",
    "        f.close()\n",
    "        \n",
    "for i in range(5):\n",
    "    identification(pd.read_csv('third_experiment/third_configuration/window_232/statistics/identification_statistics_{}_third_experiment_third_configuration_window_232.csv'.format(i)), i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
